import asyncio
import json
import os
import re
from urllib.parse import urlparse
from typing import List
import pandas as pd
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from openai import OpenAI
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from extract_shop_info import extract_shop_info
from get_place_details import get_reviews
from diving_course_normalizer import correct_diving_course_spelling
import uuid

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)

# æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
pattern = r"^(https?://)([^/]+)"

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
class Course(BaseModel):
    name: str
    price: int
    level: str

class ArticleData(BaseModel):
    course_list: List[Course] = Field(
        ..., description="ãƒ€ã‚¤ãƒ“ãƒ³ã‚°ã®ã‚³ãƒ¼ã‚¹åã¨ä¾¡æ ¼ã®ãƒªã‚¹ãƒˆ",
        examples=[
            {"name": "Open Water Diver", "price": 39800, "level": "beginner"},
            {"name": "Advanced Open Water Diver", "price": 49800, "level": "intermediate"}
        ]
    )

# å„ç¨®é–¢æ•°å®šç¾©
def load_license_data(path: str) -> tuple:
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get("license", []), data.get("specialities", [])

def sanitize_filename(url: str) -> str:
    parsed = urlparse(url)
    domain = parsed.netloc.replace('.', '_')
    path = parsed.path.strip('/').replace('/', '_') or ''
    return f"{domain}_{path}.json"

def get_web_search_prompt(hostname: str, license_list, specialty_list) -> str:
    return f"""
        ä»¥ä¸‹ã®ã‚µã‚¤ãƒˆã§æä¾›ã•ã‚Œã¦ã„ã‚‹ãƒ€ã‚¤ãƒ“ãƒ³ã‚°ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ã‚³ãƒ¼ã‚¹æƒ…å ±ã¨ãã®URLã‚’å–å¾—ã—ã¦JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚
        ãƒ»ã‚µã‚¤ãƒˆURL
        {hostname}

        ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        course_name: ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ã‚³ãƒ¼ã‚¹åã‚ã‚‹ã„ã¯ã‚¹ãƒšã‚·ãƒ£ãƒªãƒ†ã‚£ã‚³ãƒ¼ã‚¹å, examples="Open Water Diver"
        price: ã‚³ãƒ¼ã‚¹ã®ä¾¡æ ¼, examples="39800"

        ä»¥ä¸‹ã®å‡ºåŠ›å½¢å¼ã«å¾“ã„ã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹åã¨ä¾¡æ ¼ã®ãƒšã‚¢ã®å½¢ã§å…±æœ‰ã—ã¦ãã ã•ã„ã€‚

        ãƒ»å‡ºåŠ›å½¢å¼
        "course_list": [
            {{"name": "Open Water Diver", "price": 49800, "level": "beginner"}},
            {{"name": "Advanced Open Water Diver", "price": 49800, "level": "intermediate"}}
        ]

        ä»¥ä¸‹ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒªã‚¹ãƒˆã¨ã‚¹ãƒšã‚·ãƒ£ãƒªãƒ†ã‚£ãƒªã‚¹ãƒˆã¨åŒã˜å†…å®¹ãŒã‚ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã®åå‰ã«è¡¨è¨˜ã‚’æƒãˆã¦ãã ã•ã„ã€‚
        ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒªã‚¹ãƒˆ
        {license_list}
        ãƒ»ã‚¹ãƒšã‚·ãƒ£ãƒªãƒ†ã‚£ãƒªã‚¹ãƒˆ
        {specialty_list}
    """

def search_web(query):
    response = client.chat.completions.create(
        model="gpt-4o-search-preview",
        web_search_options={
            "search_context_size": "high",
            "user_location": {"type": "approximate", "approximate": {"country": "JP"}},
        },
        messages=[{"role": "user", "content": query}],
    )
    return response.choices[0].message.content

extract_prompt = """
    ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    {text}
"""

async def extract_course_info_from_url(url: str, license_list, specialty_list) -> dict:
    llm_strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(provider="openai/gpt-4.1-nano", api_token=openai_api_key),
        schema=ArticleData.model_json_schema(),
        extraction_type="schema",
        force_json_response=True,
        instruction=f"""
        ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã‹ã‚‰ã€ãƒ€ã‚¤ãƒ“ãƒ³ã‚°ã‚·ãƒ§ãƒƒãƒ—ã®ã‚³ãƒ¼ã‚¹æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        {license_list}
        {specialty_list}
        """
    )
    config = CrawlerRunConfig(
        exclude_external_links=True,
        word_count_threshold=20,
        extraction_strategy=llm_strategy,
        remove_forms=True,
        exclude_internal_links=True,
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=config)
        return json.loads(result.extracted_content)

def merge_and_clean_course_info(course_info_dict, web_search_json, target_url):
    try:
        if isinstance(course_info_dict["course_list"], dict):
            course_info_dict["course_list"] = [course_info_dict["course_list"]]
        course_info_dict["course_list"].extend(web_search_json.get("course_list", []))
        df = pd.DataFrame(course_info_dict["course_list"]).drop_duplicates(subset="name").reset_index(drop=True)
        fixed_dict = json.loads(df.to_json(orient="index"))
        course_info_dict["course_list"] = list(fixed_dict.values())
        return course_info_dict
    except Exception as e:
        print(f"âŒ course_listãƒãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e} - {target_url} ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None

def save_result(data: dict, filename: str, output_dir: str):
    os.makedirs(output_dir, exist_ok=True)
    path = os.path.join(output_dir, filename)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
    print(f"âœ… ä¿å­˜å®Œäº†: {path}")

async def process_url(target_url: str, license_list, specialty_list, output_dir: str):
    # try:
    print(f"\nğŸ” å‡¦ç†ä¸­: {target_url}")
    hostname = re.match(pattern, target_url).group(0)
    course_info_json = await extract_course_info_from_url(target_url, license_list, specialty_list)
    shop_info_json = await extract_shop_info(hostname)

    course_info_dict = course_info_json[0] if isinstance(course_info_json, list) else course_info_json
    course_name_list = license_list + specialty_list
    course_info_dict["name"] = correct_diving_course_spelling(course_info_dict["name"], client, course_name_list)
    shop_info_dict = shop_info_json[0] if isinstance(shop_info_json, list) else shop_info_json

    if "course_list" not in course_info_dict:
        course_info_dict = {"course_list": course_info_dict}

    web_search_prompt = get_web_search_prompt(hostname, license_list, specialty_list)
    course_info_text = search_web(web_search_prompt)

    response = client.chat.completions.create(
        model="gpt-4.1-nano",
        temperature=0.0,
        messages=[{"role": "user", "content": extract_prompt.format(text=course_info_text)}],
        response_format={"type": "json_object"}
    )
    web_search_json = json.loads(response.choices[0].message.content)

    merged = merge_and_clean_course_info(course_info_dict, web_search_json, target_url)
    if merged is None:
        return
    shop_info_dict.update(merged)
    reviews_dict = get_reviews(shop_info_dict["name"])
    shop_info_dict.update(reviews_dict)
    shop_info_dict.update({"id": str(uuid.uuid4())})

    filename = sanitize_filename(target_url)
    save_result(shop_info_dict, filename, output_dir)

def load_json(path: str) -> List[dict]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)
    
def merge_dive_shop_info(df: pd.DataFrame) -> pd.DataFrame:
    def merge_rows(group):
        first_row = group.iloc[0].copy()

        # course_list çµ±åˆ
        all_courses = []
        for courses in group['course_list']:
            all_courses.extend(courses)
        all_course_list = []
        name_list = []
        for course in all_courses:
            if course['name'] in name_list:
                print(f"Duplicate course name found: {course['name']}")
                continue
            all_course_list.append(course)
            name_list.append(course['name'])
        first_row['course_list'] = all_course_list
        return first_row

    merged_df = df.groupby('name', as_index=False).apply(merge_rows).reset_index(drop=True)
    return merged_df


async def main():
    license_list, specialty_list = load_license_data("backend/dive_info.json")
    output_dir = "output"
    shop_entries = load_json("backend/shop_urls.json")
    course_description = load_json("backend/course_description.json")
    for idx, entry in enumerate(shop_entries):
        url = entry["url"]
        # prefecture = entry["prefecture"]
        # name = entry["name"]
        is_checked = entry.get("is_checked", False)
        try:
            if is_checked:
                # print(f"âœ… {prefecture} - {name} ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™ã€‚")
                continue
            else:
                # print(f"ğŸ” {prefecture} - {name} ã‚’å‡¦ç†ä¸­...")
                await process_url(url, license_list, specialty_list, output_dir)
                shop_entries[idx]["is_checked"] = True
        except Exception as e:
            shop_entries[idx]["is_checked"] = False
            print(e)
            continue
    save_result(shop_entries, "backend/shop_urls.json", "./")
    path_list = os.listdir(f"{output_dir}")
    data_list = []
    for file_name in path_list:
        with open(f"{output_dir}/{file_name}", "r", encoding="utf-8") as f:
            data = json.load(f)
        data_list.append(data)
    df = pd.DataFrame(data_list)
    merged_df = merge_dive_shop_info(df)
    merged_df.to_csv("sample.csv", index=False)

if __name__ == "__main__":
    asyncio.run(main())

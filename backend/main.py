import asyncio
import json
import os
import re
from urllib.parse import urlparse
from typing import List
import pandas as pd
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from openai import OpenAI
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from database_handler import save_to_db
from extract_shop_info import extract_shop_info
from get_place_details import get_reviews
from supabase_client import add_diving_shops, add_diving_courses

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)

# æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
pattern = r"^(https?://)([^/]+)"

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
class Course(BaseModel):
    name: str
    price: int
    level: str

class ArticleData(BaseModel):
    course_list: List[Course] = Field(
        ..., description="ãƒ€ã‚¤ãƒ“ãƒ³ã‚°ã®ã‚³ãƒ¼ã‚¹åã¨ä¾¡æ ¼ã®ãƒªã‚¹ãƒˆ",
        examples=[
            {"name": "Open Water Diver", "price": 39800, "level": "beginner"},
            {"name": "Advanced Open Water Diver", "price": 49800, "level": "intermediate"}
        ]
    )

# å„ç¨®é–¢æ•°å®šç¾©
def load_license_data(path: str) -> tuple:
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get("license", []), data.get("specialty", [])

def sanitize_filename(url: str) -> str:
    parsed = urlparse(url)
    domain = parsed.netloc.replace('.', '_')
    path = parsed.path.strip('/').replace('/', '_') or ''
    return f"{domain}_{path}.json"

def get_web_search_prompt(hostname: str, license_list, specialty_list) -> str:
    return f"""
        ä»¥ä¸‹ã®ã‚µã‚¤ãƒˆã§æä¾›ã•ã‚Œã¦ã„ã‚‹ãƒ€ã‚¤ãƒ“ãƒ³ã‚°ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ã‚³ãƒ¼ã‚¹æƒ…å ±ã¨ãã®URLã‚’å–å¾—ã—ã¦JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚
        ãƒ»ã‚µã‚¤ãƒˆURL
        {hostname}

        ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        course_name: ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ã‚³ãƒ¼ã‚¹åã‚ã‚‹ã„ã¯ã‚¹ãƒšã‚·ãƒ£ãƒªãƒ†ã‚£ã‚³ãƒ¼ã‚¹å, examples="Open Water Diver"
        price: ã‚³ãƒ¼ã‚¹ã®ä¾¡æ ¼, examples="39800"

        ä»¥ä¸‹ã®å‡ºåŠ›å½¢å¼ã«å¾“ã„ã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹åã¨ä¾¡æ ¼ã®ãƒšã‚¢ã®å½¢ã§å…±æœ‰ã—ã¦ãã ã•ã„ã€‚

        ãƒ»å‡ºåŠ›å½¢å¼
        "course_list": [
            {{"name": "Open Water Diver", "price": 49800, "level": "beginner"}},
            {{"name": "Advanced Open Water Diver", "price": 49800, "level": "intermediate"}}
        ]

        ä»¥ä¸‹ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒªã‚¹ãƒˆã¨ã‚¹ãƒšã‚·ãƒ£ãƒªãƒ†ã‚£ãƒªã‚¹ãƒˆã¨åŒã˜å†…å®¹ãŒã‚ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã®åå‰ã«è¡¨è¨˜ã‚’æƒãˆã¦ãã ã•ã„ã€‚
        ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒªã‚¹ãƒˆ
        {license_list}
        ãƒ»ã‚¹ãƒšã‚·ãƒ£ãƒªãƒ†ã‚£ãƒªã‚¹ãƒˆ
        {specialty_list}
    """

def search_web(query):
    response = client.chat.completions.create(
        model="gpt-4o-search-preview",
        web_search_options={
            "search_context_size": "high",
            "user_location": {"type": "approximate", "approximate": {"country": "JP"}},
        },
        messages=[{"role": "user", "content": query}],
    )
    return response.choices[0].message.content

extract_prompt = """
    ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    {text}
"""

async def extract_course_info_from_url(url: str, license_list, specialty_list) -> dict:
    llm_strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(provider="openai/gpt-4.1-nano", api_token=openai_api_key),
        schema=ArticleData.model_json_schema(),
        extraction_type="schema",
        force_json_response=True,
        instruction=f"""
        ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã‹ã‚‰ã€ãƒ€ã‚¤ãƒ“ãƒ³ã‚°ã‚·ãƒ§ãƒƒãƒ—ã®ã‚³ãƒ¼ã‚¹æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        {license_list}
        {specialty_list}
        """
    )
    config = CrawlerRunConfig(
        exclude_external_links=True,
        word_count_threshold=20,
        extraction_strategy=llm_strategy,
        remove_forms=True,
        exclude_internal_links=True,
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=config)
        return json.loads(result.extracted_content)

def merge_and_clean_course_info(course_info_dict, web_search_json, target_url):
    try:
        if isinstance(course_info_dict["course_list"], dict):
            course_info_dict["course_list"] = [course_info_dict["course_list"]]
        course_info_dict["course_list"].extend(web_search_json.get("course_list", []))
        df = pd.DataFrame(course_info_dict["course_list"]).drop_duplicates(subset="name").reset_index(drop=True)
        fixed_dict = json.loads(df.to_json(orient="index"))
        course_info_dict["course_list"] = list(fixed_dict.values())
        return course_info_dict
    except Exception as e:
        print(f"âŒ course_listãƒãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e} - {target_url} ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None

def save_result(data: dict, filename: str, output_dir: str):
    os.makedirs(output_dir, exist_ok=True)
    path = os.path.join(output_dir, filename)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
    print(f"âœ… ä¿å­˜å®Œäº†: {path}")

async def process_url(target_url: str, license_list, specialty_list, output_dir: str):
    # try:
    print(f"\nğŸ” å‡¦ç†ä¸­: {target_url}")
    hostname = re.match(pattern, target_url).group(0)
    course_info_json = await extract_course_info_from_url(target_url, license_list, specialty_list)
    shop_info_json = await extract_shop_info(hostname)

    course_info_dict = course_info_json[0] if isinstance(course_info_json, list) else course_info_json
    shop_info_dict = shop_info_json[0] if isinstance(shop_info_json, list) else shop_info_json

    if "course_list" not in course_info_dict:
        course_info_dict = {"course_list": course_info_dict}

    web_search_prompt = get_web_search_prompt(hostname, license_list, specialty_list)
    course_info_text = search_web(web_search_prompt)

    response = client.chat.completions.create(
        model="gpt-4.1-nano",
        temperature=0.0,
        messages=[{"role": "user", "content": extract_prompt.format(text=course_info_text)}],
        response_format={"type": "json_object"}
    )
    web_search_json = json.loads(response.choices[0].message.content)

    merged = merge_and_clean_course_info(course_info_dict, web_search_json, target_url)
    if merged is None:
        return
    shop_info_dict.update(merged)
    if shop_info_dict.get("name"):
        reviews_dict = get_reviews(shop_info_dict["name"])
        if reviews_dict:
            shop_info_dict.update(reviews_dict)

    filename = sanitize_filename(target_url)
    save_result(shop_info_dict, filename, output_dir)

def load_json(path: str) -> List[dict]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)
    
def merge_dive_shop_info(df: pd.DataFrame) -> pd.DataFrame:
    def merge_rows(group):
        first_row = group.iloc[0].copy()

        # course_list çµ±åˆ
        all_courses = []
        for courses in group['course_list']:
            all_courses.extend(courses)
        all_course_list = []
        name_list = []
        for course in all_courses:
            if course['name'] in name_list:
                print(f"Duplicate course name found: {course['name']}")
                continue
            all_course_list.append(course)
            name_list.append(course['name'])
        first_row['course_list'] = all_course_list
        return first_row

    merged_df = df.groupby('name', as_index=False).apply(merge_rows).reset_index(drop=True)
    return merged_df


async def main():
    license_list, specialty_list = load_license_data("backend/dive_info.json")
    output_dir = "output"
    shop_urls_path = "backend/shop_urls.json"
    shop_status_path = "backend/shop_status.json"

    shop_entries = load_json(shop_urls_path)

    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆãªã‘ã‚Œã°ç©ºã®è¾æ›¸ï¼‰
    try:
        with open(shop_status_path, "r", encoding="utf-8") as f:
            shop_status = json.load(f)
    except FileNotFoundError:
        shop_status = {}

    for entry in shop_entries:
        url = entry["url"]
        # URLã‚’ã‚­ãƒ¼ã¨ã—ã¦ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
        if shop_status.get(url):
            print(f"âœ… {entry.get('name', url)} ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        try:
            print(f"ğŸ” {entry.get('name', url)} ã‚’å‡¦ç†ä¸­...")
            await process_url(url, license_list, specialty_list, output_dir)
            # å‡¦ç†æˆåŠŸå¾Œã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            shop_status[url] = True
        except Exception as e:
            print(f"âŒ {entry.get('name', url)} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°ã—ãªã„ï¼ˆãƒªãƒˆãƒ©ã‚¤å¯èƒ½ã«ã™ã‚‹ï¼‰
            continue
        finally:
            # å®šæœŸçš„ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€
            with open(shop_status_path, "w", encoding="utf-8") as f:
                json.dump(shop_status, f, indent=4, ensure_ascii=False)

    print("\nâœ¨ ã™ã¹ã¦ã®URLã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # --- å¾Œç¶šå‡¦ç†ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ã€DBä¿å­˜ã€CSVå‡ºåŠ›ï¼‰---
    path_list = [p for p in os.listdir(output_dir) if p.endswith('.json')]
    if not path_list:
        print("\nâš ï¸ å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å¾Œç¶šå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    data_list = []
    for file_name in path_list:
        with open(os.path.join(output_dir, file_name), "r", encoding="utf-8") as f:
            data = json.load(f)
        data_list.append(data)

    df = pd.DataFrame(data_list)
    merged_df = merge_dive_shop_info(df)

    # DBä¿å­˜
    save_to_db(merged_df)

    # CSVå‡ºåŠ›
    output_csv_path = os.path.join(output_dir, "merged_df.csv")
    merged_df.to_csv(output_csv_path, index=False)
    print(f"\nğŸ“„ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {output_csv_path}")

if __name__ == "__main__":
    asyncio.run(main())
